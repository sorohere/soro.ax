<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>soro | vad</title>
    <link rel="icon" type="image/jpeg" href="../images/soro.jpg">
    <link rel="stylesheet" href="../posts/blogs.css">
</head>
<body>
    <div class="soro">
        soro
    </div>
    <div class="breadcrumbs"></div><br>
    <h2>The Art of Detecting Speech</h2>

    <p>
        This post explores <a href="https://drive.google.com/file/d/1JeR6GuQ4KrDOmjwPcNsoJAUJVIcOYJHE/view?usp=sharing" target="_blank">my recent work</a> on Voice Activity Detection (VAD). Rather than diving into technical stuff, the goal is to explain the concept in an easy way, with a touch of humor to keep it engaging. Let’s get into it!
    </p>
    

    <h3>What’s the Big Deal with Voice Activity Detection?</h3>

    <p>Imagine you’re at a party, and you’re trying to listen to your friend’s story about their cat. But, oh no! The music is blasting, people are laughing, and someone just dropped a plate. How do you focus on your friend’s voice in all that chaos?
        <br>
        Well, that’s exactly what VAD does, it helps machines figure out when someone is speaking and when they’re not, even in noisy environments.
        <br><br>
        VAD is like the bouncer at a club, deciding who gets in (speech) and who stays out (noise). It’s a crucial part of speech processing systems, from voice assistants like <a href="https://machinelearning.apple.com/research/voice-trigger" target="_blank">Siri</a> and <a href="https://developer.amazon.com/en-US/docs/alexa/ask-overviews/what-is-the-alexa-skills-kit.html" target="_blank">Alexa</a> to transcription services and even call centers. But here’s the kicker not all VAD systems are created equal. Some are great at detecting speech, while others… well, let’s just say they need a bit more training.
    </p>

    <h3>The Traditional vs. Modern Showdown</h3>

    <p>
        In my research, I looked at two main ways to evaluate VAD systems: traditional metrics and modern metrics. Think of it like judging a pizza: traditional metrics are like checking if the pizza has the right amount of cheese and toppings, while modern metrics are like tasting it to see if it’s actually delicious.
            <!-- Table for Images -->
        <table class="image-table">
            <tr>
                <td>
                    <img src="../images/bg03-01.png" alt="Perceptron Diagram" class="responsive-image">
                    <p class="image-caption">measuring the cheese and toppings, a traditional way.</p>
                </td>
                <td>
                    <img src="../images/bg03-02.png" alt="Gradient Descent Diagram" class="responsive-image">
                    <p class="image-caption">consideration of boundaries of speech. (source: <a href="https://www.newscientist.com/">newscientist.com</a>)</p>
                </td>
            </tr>
        </table>
        
        <br><br><br>
        <strong>Traditional Metrics: The Cheese and Toppings</strong>
        <br>
        Traditional metrics are all about numbers. They measure things like:
        <ul>
            <li><strong><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html" target="_blank">Precision</a></strong>: How often the system correctly identifies speech.</li>
            <li><strong><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html" target="_blank">Recall</a></strong>: How much speech the system actually catches.</li>
            <li><strong><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html" target="_blank">F1-Score</a></strong>: A balance between precision and recall (because who doesn’t love a good balance?).</li>
            <li><strong><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html" target="_blank">Accuracy</a></strong>: Overall, how good the system is at telling speech from noise.</li>
        </ul>
        These metrics are great, but they have a blind spot, they don’t care much about the boundaries of speech. Imagine if you were cutting a pizza, and you accidentally cut off a slice of the crust. Traditional metrics wouldn’t notice, but modern metrics definitely would.
    </p>

    <p>
        <strong>Modern Metrics: The Taste Test</strong>
        <br>
        Modern metrics are more nuanced. They focus on things like:
        <ul>
            <li><strong>Front End Clipping <a href="https://en.wikipedia.org/wiki/Voice_activity_detection#Performance_evaluation" target="_blank">(FEC)</a></strong>: When the system misses the start of a sentence (like cutting off the first bite of pizza).</li>
            <li><strong>Mid-Speech Clipping <a href="https://arrow.tudublin.ie/cgi/viewcontent.cgi?article=1162&context=scschcomcon" target="_blank">(MSC)</a></strong>: When the system cuts out parts of the middle of a sentence (like missing the gooey cheese in the middle).</li>
            <li><strong>Over Hang <a href="https://israelcohen.com/wp-content/uploads/2018/05/TASLP_June2013.pdf" target="_blank">(OVER)</a></strong>: When the system thinks noise is speech (like mistaking a pepper flake for a topping).</li>
            <li><strong>Noise Detected as Speech <a href="https://arxiv.org/pdf/2410.14509" target="_blank">(NDS)</a></strong>: When the system thinks background noise is speech (like thinking the sound of the oven is part of the pizza).</li>
        </ul>
        These metrics give us a better idea of how well the VAD system performs in real world scenarios, where things are messy and unpredictable: just like my kitchen when I’m trying to cook.
        
    </p>

    <p>
        <strong>The Datasets: Bhojpuri and Vani</strong>
        <br><br>
        To test these VAD systems, I used two datasets: the <a href="https://github.com/shashwatup9k/bho-resources/tree/master" target="_blank">Bhojpuri dataset</a> (because why not?) and the <a href="https://vaani.iisc.ac.in/#Data" target="_blank">Vani dataset</a>, which includes audio samples from nine different Indic languages. It’s like testing pizza in different cities, each place has its own unique flavor.
        <br>
        The results? Well, let’s just say some systems were like gourmet chefs, while others were more like college students trying to make ramen.
    </p>

    <h3>The Winners and Losers</h3>

    <p>
        After running the tests, a few models stood out:
        <ul>
            <li><a href="https://github.com/snakers4/silero-vad" target="_blank">Silero</a>: This model was the MVP in traditional metrics, acing precision, recall, and accuracy. It’s like the pizza chef who always gets the cheese-to-sauce ratio perfect.</li>
            <li><a href="https://huggingface.co/speechbrain/vad-crdnn-libriparty" target="_blank">SpeechBrain</a>: This one shined in modern metrics, especially in boundary detection. It’s like the chef who knows exactly when to take the pizza out of the oven—no burnt crusts here!</li>
            <li><a href="https://huggingface.co/pyannote/voice-activity-detection" target="_blank">Pyannote</a> and <a href="https://huggingface.co/funasr/fsmn-vad" target="_blank">FunASR</a>: These models are solid performers, nothing groundbreaking, but definitely not slouches either. They’re like a dependable neighborhood pizza spot: not the best in town, but you’ll rarely be disappointed.</li>
        </ul>
    </p>

    <h3>The <a href="https://speechprocessingbook.aalto.fi/Recognition/Voice_activity_detection.html#performance-in-noise-3db-4db-threshold" target="_blank">SNR</a> Experiment: Turning Up the Noise</h3>

    <p>
        To make things even more interesting, I added uneven noise to the audio samples (because who doesn’t love a challenge?). The goal was to see how well these VAD systems could handle real world noise levels. 
    </p>

    <img src="../images/bg03-03.png">

    <p>
        Spoiler alert: after a certain point, the systems got better, but there’s only so much noise they can handle before they start to crack, kind of like me trying to focus on a conversation while in sleepy af.
    </p>

    <h3>The Takeaway: Which Model Should You Choose?</h3>
    <p>
        Choosing the right VAD system depends on your specific needs. Different models excel in different scenarios, from simple single-speaker audio to complex multi-speaker environments.
        <ul>
            <li>If you're working on a small project where your audio contains only a single speech segment, use FunASR.</li>
            <li>If your audio has multiple speech segments with varying pitches, SpeechBrain and Silero are the best choices. If you can tolerate little loss, Pyanote can be used as well.</li>
            <li>If your goal is speaker diarization: separating and identifying different speakers in an audio file, Pyannote is a strong choice.</li>
        </ul>

        <img src="../images/bg03-04.png">
        
        The choice of a speech model depends on your use case. A voice assistant needs minimal FEC for smooth interactions. while lecture transcription requires handling multiple speakers and varying pitch SpeechBrain or Silero work well here. FunASR is great for single segment speech, while Pyanote can be used for diarization with some trade-offs. Choose based on accuracy, latency, and complexity.
    </p>


    <h3>The ASR Comparison: When VAD Meets Its Match</h3>

    <p>
        I also compared the VAD systems with an Automatic Speech Recognition (ASR) model. Think of ASR as the ultimate pizza critic, it knows exactly where the speech starts and ends. The comparison revealed some interesting gaps, especially in how the VAD systems handled noise. It’s like realizing your favorite pizza place sometimes forgets to add the pepperoni.
    </p>

    <h3>Final Thoughts</h3>

    <p>
        Voice Activity Detection might sound like a dry topic, but it’s actually pretty fascinating when you think about it. It’s all about teaching machines to listen like humans do, filtering out the noise and focusing on what matters. And just like with pizza, there’s no one-size-fits-all solution. It’s all about finding the right balance.
        <br><br>
        So, the next time you’re talking to Siri or Alexa, remember, there’s a whole world of VAD magic happening behind the scenes. And who knows? Maybe one day, VAD systems will be so good, they’ll even understand my terrible jokes.
    </p>

    <p>Thanks for sticking around! Until next time, keep <a href="https://www.youtube.com/watch?v=OPf0YbXqDm0&list=PLGBuKfnErZlB3AThAEKz8_3kbYTocgfbB" target="_blank">listening</a>.</p>

    <script src="../mains.js"></script>

    <footer>
        <div class="footer-content">
            <p>&copy; 2025, Built with love and gpt's.</p>
        </div>
    </footer>
</body>
</html>

